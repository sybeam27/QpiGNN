{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.normal import Normal\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv, GraphSAGE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_data(num_nodes=1000):\n",
    "    np.random.seed(1127)\n",
    "    torch.manual_seed(1127)\n",
    "    \n",
    "    # 비선형 데이터\n",
    "    X = np.random.rand(num_nodes, 5)  # 5차원\n",
    "    y = np.sin(X[:, 0] * 3) + 0.1 * np.random.rand(num_nodes)\n",
    "    \n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))\n",
    "    \n",
    "    # PyG 데이터 변환\n",
    "    return Data(\n",
    "        x = torch.tensor(X, dtype = torch.float32),\n",
    "        edge_index= edge_index,\n",
    "        y = torch.tensor(y, dtype = torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "def generate_noisy_graph_data(num_nodes=1000, noise_type=\"gaussian\", noise_level=0.1, outlier_ratio=0.05):\n",
    "    \"\"\"\n",
    "    다양한 노이즈를 추가하여 그래프 데이터를 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "    - num_nodes (int): 노드 개수\n",
    "    - noise_type (str): 추가할 노이즈 유형 (\"gaussian\", \"uniform\", \"outlier\", \"edge_noise\")\n",
    "    - noise_level (float): 노이즈의 강도 (가우시안 및 유니폼 노이즈)\n",
    "    - outlier_ratio (float): 이상치(outlier) 비율\n",
    "\n",
    "    Returns:\n",
    "    - PyG Data 객체\n",
    "    \"\"\"\n",
    "    np.random.seed(1127)\n",
    "    torch.manual_seed(1127)\n",
    "    \n",
    "    X = np.random.rand(num_nodes, 5)  # 5차원 특징\n",
    "    y = np.sin(X[:, 0] * 3) + 0.1 * np.random.rand(num_nodes)  # 기본 타겟\n",
    "    \n",
    "    if noise_type == \"gaussian\":\n",
    "        y += np.random.normal(0, noise_level, size=num_nodes)\n",
    "    elif noise_type == \"uniform\":\n",
    "        y += np.random.uniform(-noise_level, noise_level, size=num_nodes)\n",
    "    elif noise_type == \"outlier\":\n",
    "        num_outliers = int(num_nodes * outlier_ratio)\n",
    "        outlier_indices = np.random.choice(num_nodes, num_outliers, replace=False)\n",
    "        y[outlier_indices] += np.random.normal(3, 1.0, size=num_outliers)  # 극단적인 변화\n",
    "\n",
    "    # 그래프 구조적 노이즈 (엣지 변경)\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))\n",
    "    if noise_type == \"edge_noise\":\n",
    "        # 엣지에 무작위 잡음을 추가하여 구조적 변형 수행\n",
    "        num_noisy_edges = int(edge_index.shape[1] * noise_level)\n",
    "        noise_indices = np.random.choice(edge_index.shape[1], num_noisy_edges, replace=False)\n",
    "        edge_index[:, noise_indices] = torch.randint(0, num_nodes, (2, num_noisy_edges))\n",
    "\n",
    "    return Data(\n",
    "        x=torch.tensor(X, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "def max_normalize(x):\n",
    "    return x / np.max(np.abs(x)) if np.max(np.abs(x)) != 0 else x\n",
    "\n",
    "def std_normalize(x):\n",
    "    return (x - np.mean(x)) / np.std(x) if np.std(x) != 0 else np.zeros(len(x))\n",
    "\n",
    "def int_normalize(x):\n",
    "    return ((x - np.min(x)) / (np.max(x) - np.min(x)) * 2 - 1) if np.std(x) != 0 else np.zeros(len(x))\n",
    "\n",
    "def simulate_ising(n, h0, J):\n",
    "    G = nx.grid_2d_graph(n, n)\n",
    "    l = np.linspace(-1.0, 1.0, n)\n",
    "    \n",
    "    s = np.random.choice([-1, 1], size=(n, n))\n",
    "    # Placeholder for metropolis algorithm\n",
    "    y = s.flatten()\n",
    "    f = [[l[i], l[j]] for j in range(n) for i in range(n)]\n",
    "    \n",
    "    return G, [nx.to_scipy_sparse_matrix(G)], y, f\n",
    "\n",
    "def parse_mean_fill(series, normalize=False):\n",
    "    series = series.replace({',': ''}, regex=True)\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    mean_val = series.mean()\n",
    "    series.fillna(mean_val, inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "        series = (series - mean_val) / series.std()\n",
    "    \n",
    "    return series.values\n",
    "\n",
    "def read_county(prediction, year):\n",
    "    adj = pd.read_csv(\"dataset/election/adjacency.txt\", header=None, sep=\"\\t\", dtype=str, encoding=\"ISO-8859-1\")\n",
    "    fips2cty = {row[1]: row[0] for _, row in adj.iterrows() if pd.notna(row[1])}\n",
    "    \n",
    "    hh = adj.iloc[:, 1].ffill().astype(int)\n",
    "    tt = adj.iloc[:, 3].astype(int)\n",
    "    \n",
    "    fips = sorted(set(hh).union(set(tt)))\n",
    "    id2num = {id_: num for num, id_ in enumerate(fips)}\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(id2num)))\n",
    "    G.add_edges_from([(id2num[h], id2num[t]) for h, t in zip(hh, tt)])\n",
    "    \n",
    "    # Load datasets\n",
    "    VOT = pd.read_csv(\"dataset/election/election.csv\")\n",
    "    ICM = pd.read_csv(\"dataset/election/income.csv\")\n",
    "    POP = pd.read_csv(\"dataset/election/population.csv\")\n",
    "    EDU = pd.read_csv(\"dataset/election/education.csv\")\n",
    "    UEP = pd.read_csv(\"dataset/election/unemployment.csv\")\n",
    "    \n",
    "    cty = pd.DataFrame({'FIPS': fips, 'County': [fips2cty.get(f, '') for f in fips]})\n",
    "    vot = VOT[['fips_code', f'dem_{year}', f'gop_{year}']].rename(columns={'fips_code': 'FIPS'})\n",
    "    icm = ICM[['FIPS', f'MedianIncome{min(max(2011, year), 2018)}']]\n",
    "    pop = POP[['FIPS', f'R_NET_MIG_{min(max(2011, year), 2018)}', f'R_birth_{min(max(2011, year), 2018)}', f'R_death_{min(max(2011, year), 2018)}']]\n",
    "    edu = EDU[['FIPS', f'BachelorRate{year}']]\n",
    "    uep = UEP[['FIPS', f'Unemployment_rate_{min(max(2007, year), 2018)}']]\n",
    "    \n",
    "    dat = cty.merge(vot, on='FIPS', how='left')\n",
    "    dat = dat.merge(icm, on='FIPS', how='left')\n",
    "    dat = dat.merge(pop, on='FIPS', how='left')\n",
    "    dat = dat.merge(edu, on='FIPS', how='left')\n",
    "    dat = dat.merge(uep, on='FIPS', how='left')\n",
    "    \n",
    "    # Extract features and labels\n",
    "    dem = parse_mean_fill(dat.iloc[:, 2])\n",
    "    gop = parse_mean_fill(dat.iloc[:, 3])\n",
    "    \n",
    "    ff = np.zeros((len(dat), 7), dtype=np.float32)\n",
    "    for i in range(6):\n",
    "        ff[:, i] = parse_mean_fill(dat.iloc[:, i + 4], normalize=True)\n",
    "    \n",
    "    ff[:, 6] = (gop - dem) / (gop + dem)\n",
    "    \n",
    "    label_mapping = {\n",
    "        \"income\": 0, \"migration\": 1, \"birth\": 2, \"death\": 3,\n",
    "        \"education\": 4, \"unemployment\": 5, \"election\": 6\n",
    "    }\n",
    "    \n",
    "    if prediction not in label_mapping:\n",
    "        raise ValueError(\"Unexpected prediction type\")\n",
    "    \n",
    "    pos = label_mapping[prediction]\n",
    "    y = ff[:, pos]\n",
    "    f = [np.concatenate((ff[i, :pos], ff[i, pos + 1:])) for i in range(len(dat))]\n",
    "    \n",
    "    return G, [csr_matrix(nx.adjacency_matrix(G))], y, f\n",
    "\n",
    "def load_county_graph_data(prediction: str, year: int):\n",
    "    G, A, labels, feats = read_county(prediction, year)\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def read_transportation_network(network_name, net_skips, net_cols, netf_cols, flow_skips, flow_cols, V_range):\n",
    "    # Load data\n",
    "    dat_net = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_net.tntp\", \n",
    "                           skiprows=net_skips, sep='\\s+', usecols=net_cols, header=None).values\n",
    "    dat_netf = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_net.tntp\", \n",
    "                            skiprows=net_skips, sep='\\s+', usecols=netf_cols, header=None).values\n",
    "    dat_flow = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_flow.tntp\", \n",
    "                            skiprows=flow_skips, sep='\\s+', usecols=flow_cols, header=None).values\n",
    "    \n",
    "    # Map node labels to indices\n",
    "    lb2id = {v: i for i, v in enumerate(V_range, start=1)}\n",
    "    NV = len(V_range)\n",
    "    \n",
    "    # Create directed graph\n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(range(1, NV + 1))\n",
    "    \n",
    "    for src, dst in dat_net:\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            g.add_edge(lb2id[src], lb2id[dst])\n",
    "    \n",
    "    # Edge labels\n",
    "    flow_dict = {}\n",
    "    for src, dst, flow in dat_flow:\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            flow_dict[(lb2id[src], lb2id[dst])] = flow\n",
    "    \n",
    "    y = np.array([flow_dict.get((e[0], e[1]), 0) for e in g.edges()])\n",
    "    y = (y - np.mean(y)) / np.std(y)  # Standard normalization\n",
    "    \n",
    "    # Edge features\n",
    "    netf_dict = {}\n",
    "    for i in range(len(dat_net)):\n",
    "        src, dst = dat_net[i]\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            netf_dict[(lb2id[src], lb2id[dst])] = dat_netf[i]\n",
    "    \n",
    "    ff = np.array([netf_dict[e] for e in g.edges()])\n",
    "    mean_ff = np.mean(ff, axis=0)\n",
    "    std_ff = np.std(ff, axis=0)\n",
    "    std_ff[std_ff == 0] = 1  # Prevent division by zero\n",
    "    netf = (ff - mean_ff) / std_ff  \n",
    "    \n",
    "    f = list(netf)\n",
    "    \n",
    "    # Line graph transformation\n",
    "    G1 = nx.Graph()\n",
    "    G2 = nx.Graph()\n",
    "    sorted_edges = sorted(g.edges())\n",
    "    tuple2id = {e: i for i, e in enumerate(sorted_edges)}\n",
    "    \n",
    "    for u in g.nodes:\n",
    "        innbrs = list(g.predecessors(u))\n",
    "        outnbrs = list(g.successors(u))\n",
    "        \n",
    "        for v in innbrs:\n",
    "            for w in outnbrs:\n",
    "                if (v, u) in tuple2id and (u, w) in tuple2id:\n",
    "                    G1.add_edge(tuple2id[(v, u)], tuple2id[(u, w)])\n",
    "        \n",
    "        for v in innbrs:\n",
    "            for w in innbrs:\n",
    "                if w > v and (v, u) in tuple2id and (w, u) in tuple2id:\n",
    "                    G2.add_edge(tuple2id[(v, u)], tuple2id[(w, u)])\n",
    "        \n",
    "        for v in outnbrs:\n",
    "            for w in outnbrs:\n",
    "                if w > v and (u, v) in tuple2id and (u, w) in tuple2id:\n",
    "                    G2.add_edge(tuple2id[(u, v)], tuple2id[(u, w)])\n",
    "                    \n",
    "    size = max(len(G1.nodes), len(G2.nodes))\n",
    "    A1 = np.zeros((size, size))\n",
    "    A2 = np.zeros((size, size))\n",
    "    \n",
    "    A1[:nx.number_of_nodes(G1), :nx.number_of_nodes(G1)] = nx.adjacency_matrix(G1).todense()\n",
    "    A2[:nx.number_of_nodes(G2), :nx.number_of_nodes(G2)] = nx.adjacency_matrix(G2).todense()\n",
    "    \n",
    "    A = A1 + A2\n",
    "    \n",
    "    return nx.Graph(A), A, y, f\n",
    "\n",
    "def load_trans_graph_data(city: str):\n",
    "    if city == 'Anaheim':\n",
    "        G, A, labels, feats = read_transportation_network(city, 8, [0, 1], [2, 3, 4, 7], 6, [0, 1, 3], range(1, 417))\n",
    "    elif city == 'ChicagoSketch':\n",
    "        G, A, labels, feats = read_transportation_network(city, 7, [0, 1], [2, 3, 4, 7], 1, [0, 1, 2], range(388, 934))\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def read_twitch_network(cnm, dim_reduction=False, dim_embed=8):\n",
    "    feats_all = []\n",
    "    countries = [\"DE\", \"ENGB\", \"ES\", \"FR\", \"PTBR\", \"RU\"]\n",
    "    \n",
    "    for cn in countries:\n",
    "        with open(f\"dataset/twitch/{cn}/musae_{cn}_features.json\", \"r\") as f:\n",
    "            feats = json.load(f)\n",
    "        feats_all.extend(feats.values())\n",
    "\n",
    "    ndim = max(np.concatenate(feats_all)) + 1\n",
    "\n",
    "    def feat_encode(feat_list):\n",
    "        \"\"\"특징 벡터를 원핫 인코딩 형태로 변환\"\"\"\n",
    "        vv = np.zeros(ndim, dtype=np.float32)\n",
    "        valid_indices = np.array(feat_list)\n",
    "        \n",
    "        if np.any(valid_indices >= ndim):\n",
    "            raise ValueError(f\"Index out of bounds! Max index: {max(valid_indices)}, ndim: {ndim}\")\n",
    "        \n",
    "        vv[valid_indices] = 1.0\n",
    "        return vv\n",
    "\n",
    "    f_all = list(map(feat_encode, feats_all))\n",
    "\n",
    "    with open(f\"dataset/twitch/{cnm}/musae_{cnm}_features.json\", \"r\") as f:\n",
    "        feats = json.load(f)\n",
    "\n",
    "    id2ft = {int(k) + 1: v for k, v in feats.items()}\n",
    "    n = len(id2ft)\n",
    "    assert min(id2ft.keys()) == 1 and max(id2ft.keys()) == n\n",
    "\n",
    "    f = [feat_encode(id2ft[i]) for i in sorted(id2ft.keys())]\n",
    "\n",
    "    if dim_reduction:\n",
    "        f_matrix = np.stack(f_all, axis=1)\n",
    "        U, S, Vt = svds(f_matrix, k=dim_embed)\n",
    "        U *= np.sign(np.sum(U, axis=0))  # sign correction\n",
    "        f = [U.T @ f_ for f_ in f]\n",
    "\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(range(1, len(f) + 1))\n",
    "\n",
    "    links = pd.read_csv(f\"dataset/twitch/{cnm}/musae_{cnm}_edges.csv\")\n",
    "    for _, row in links.iterrows():\n",
    "        g.add_edge(row[\"from\"] + 1, row[\"to\"] + 1)\n",
    "\n",
    "    trgts = pd.read_csv(f\"dataset/twitch/{cnm}/musae_{cnm}_target.csv\")\n",
    "    nid2views = dict(zip(trgts[\"new_id\"], trgts[\"views\"]))\n",
    "    y = std_normalize(np.log([nid2views[i - 1] + 1.0 for i in range(1, g.number_of_nodes() + 1)]))\n",
    "\n",
    "    return g, [csr_matrix(nx.adjacency_matrix(g))], y, f\n",
    "\n",
    "def load_twitch_graph_data(cnm: str):\n",
    "    G, A, labels, feats = read_twitch_network(cnm)\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def load_wiki_graph_data(category):\n",
    "    edge_path = f'dataset/wikipedia/{category}/musae_{category}_edges.csv'\n",
    "    feature_path = f'dataset/wikipedia/{category}/musae_{category}_features.json'\n",
    "    target_path = f'dataset/wikipedia/{category}/musae_{category}_target.csv'\n",
    "    \n",
    "    # 엣지 데이터 로드\n",
    "    edge_df = pd.read_csv(edge_path)\n",
    "    edge_index = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "    \n",
    "    # 피처 데이터 로드\n",
    "    with open(feature_path, \"r\") as f:\n",
    "        features_dict = json.load(f)\n",
    "    \n",
    "    node_ids = sorted(map(int, features_dict.keys()))  # 노드 ID 정렬\n",
    "    node_id_map = {old_id: new_id for new_id, old_id in enumerate(node_ids)}\n",
    "    \n",
    "    num_nodes = len(node_ids)\n",
    "    num_features = max(max(v) for v in features_dict.values()) + 1  # 가장 큰 feature index 찾기\n",
    "    x = torch.zeros((num_nodes, num_features), dtype=torch.float32)\n",
    "    \n",
    "    for node, features in features_dict.items():\n",
    "        new_id = node_id_map[int(node)]  # 노드 ID 변환\n",
    "        x[new_id, features] = 1.0  # One-hot 인코딩\n",
    "    \n",
    "    # 타겟 데이터 로드\n",
    "    target_df = pd.read_csv(target_path)\n",
    "    target_df[\"id\"] = target_df[\"id\"].map(node_id_map)  # 노드 ID 변환\n",
    "    target_df = target_df.dropna().astype(int)  # 변환되지 않은 노드 제거\n",
    "    \n",
    "    # y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    y = torch.zeros((num_nodes, 1), dtype=torch.long)  # [, 1] 형태로 변경\n",
    "    y[target_df[\"id\"].values] = torch.tensor(target_df[\"target\"].values, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    # PyG Data 객체 생성\n",
    "    graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    return graph_data\n",
    "\n",
    "def set_seed(seed=1127):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"PyTorch Tensor → NumPy 변환 후 1차원으로 변형\"\"\"\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        return tensor.cpu().numpy().squeeze()\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.squeeze()\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(tensor)}\")\n",
    " \n",
    "def sort_by_y(x, y, *intervals):\n",
    "    sort_idx = np.argsort(to_numpy(y).ravel())  # Y값을 기준으로 정렬할 인덱스\n",
    "    sorted_x = to_numpy(x).ravel()[sort_idx]\n",
    "    sorted_y = to_numpy(y).ravel()[sort_idx]\n",
    "    sorted_intervals = [to_numpy(interval).ravel()[sort_idx] for interval in intervals]\n",
    "    return sorted_x, sorted_y, sorted_intervals\n",
    "\n",
    "def split_graph_data(graph_data, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    GNN용 Train-Test Split (edge_index를 올바르게 재매핑하여 유지)\n",
    "    \"\"\"\n",
    "    num_nodes = graph_data.x.shape[0]  # 전체 노드 개수\n",
    "    num_test = int(num_nodes * test_ratio)  # 테스트 데이터 노드 개수\n",
    "\n",
    "    # 랜덤하게 Train/Test 노드 인덱스 선택\n",
    "    indices = torch.randperm(num_nodes)\n",
    "    test_nodes = indices[:num_test]\n",
    "    train_nodes = indices[num_test:]\n",
    "\n",
    "    # 노드 마스크 생성\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_nodes] = True\n",
    "    test_mask[test_nodes] = True\n",
    "\n",
    "    # Train/Test용 edge_index 필터링\n",
    "    train_edge_mask = train_mask[graph_data.edge_index[0]] & train_mask[graph_data.edge_index[1]]\n",
    "    test_edge_mask = test_mask[graph_data.edge_index[0]] & test_mask[graph_data.edge_index[1]]\n",
    "\n",
    "    # Train용 edge_index와 노드 인덱스 재매핑\n",
    "    train_edge_index = graph_data.edge_index[:, train_edge_mask]\n",
    "    train_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(train_nodes)}\n",
    "    train_edge_index = torch.tensor(\n",
    "        [[train_node_map[idx.item()] for idx in train_edge_index[0]],\n",
    "         [train_node_map[idx.item()] for idx in train_edge_index[1]]],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Test용 edge_index와 노드 인덱스 재매핑\n",
    "    test_edge_index = graph_data.edge_index[:, test_edge_mask]\n",
    "    test_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(test_nodes)}\n",
    "    test_edge_index = torch.tensor(\n",
    "        [[test_node_map[idx.item()] for idx in test_edge_index[0]],\n",
    "         [test_node_map[idx.item()] for idx in test_edge_index[1]]],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Train 데이터 생성\n",
    "    train_data = Data(\n",
    "        x=graph_data.x[train_mask],\n",
    "        y=graph_data.y[train_mask],\n",
    "        edge_index=train_edge_index\n",
    "    )\n",
    "\n",
    "    # Test 데이터 생성\n",
    "    test_data = Data(\n",
    "        x=graph_data.x[test_mask],\n",
    "        y=graph_data.y[test_mask],\n",
    "        edge_index=test_edge_index\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Train Nodes: {train_data.x.shape[0]}, Train Edges: {train_data.edge_index.shape[1]}\")\n",
    "    print(f\"Train edge_index 최대값: {train_data.edge_index.max().item()}\")\n",
    "    print(f\"Test Nodes: {test_data.x.shape[0]}, Test Edges: {test_data.edge_index.shape[1]}\")\n",
    "    print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# def split_graph_data(graph_data, test_ratio=0.2):\n",
    "#     transform = RandomNodeSplit(split=\"train_rest\", num_val=0.0, num_test=test_ratio)\n",
    "#     data = transform(graph_data)\n",
    "\n",
    "#     # Train/Test 마스크 생성\n",
    "#     train_mask = data.train_mask.to(data.x.device)\n",
    "#     test_mask = data.test_mask.to(data.x.device)\n",
    "\n",
    "#     # 훈련 및 테스트 데이터 선택\n",
    "#     train_data = data.clone()\n",
    "#     test_data = data.clone()\n",
    "\n",
    "#     train_data.x = data.x[train_mask]\n",
    "#     train_data.y = data.y[train_mask]\n",
    "#     test_data.x = data.x[test_mask]\n",
    "#     test_data.y = data.y[test_mask]\n",
    "\n",
    "#     # 노드 인덱스를 매핑하여 edge_index 수정 (훈련 데이터)\n",
    "#     train_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(train_mask.nonzero(as_tuple=True)[0])}\n",
    "#     train_edges_mask = train_mask[data.edge_index[0]] & train_mask[data.edge_index[1]]\n",
    "#     train_edges = data.edge_index[:, train_edges_mask]\n",
    "\n",
    "#     train_data.edge_index = torch.stack([\n",
    "#         torch.tensor([train_node_mapping[i.item()] for i in train_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "#         torch.tensor([train_node_mapping[i.item()] for i in train_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "#     ], dim=0)\n",
    "\n",
    "#     # 노드 인덱스를 매핑하여 edge_index 수정 (테스트 데이터)\n",
    "#     test_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(test_mask.nonzero(as_tuple=True)[0])}\n",
    "#     test_edges_mask = test_mask[data.edge_index[0]] & test_mask[data.edge_index[1]]\n",
    "#     test_edges = data.edge_index[:, test_edges_mask]\n",
    "\n",
    "#     test_data.edge_index = torch.stack([\n",
    "#         torch.tensor([test_node_mapping[i.item()] for i in test_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "#         torch.tensor([test_node_mapping[i.item()] for i in test_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "#     ], dim=0)\n",
    "\n",
    "#     print(f\"Train data: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges\")\n",
    "#     print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "    \n",
    "#     return train_data, test_data\n",
    "\n",
    "def augment_features(x, tau):\n",
    "    if isinstance(tau, float):  # tau가 float이면 변환\n",
    "        tau = torch.tensor([tau])\n",
    "    \n",
    "    tau = tau.view(-1, 1)\n",
    "    tau_transformed = (tau - 0.5) * 12 # 분위수 값 변환: 학습 안정성 증가\n",
    "    \n",
    "    return torch.cat((x, tau_transformed.expand(x.size(0), -1)), dim = 1)\n",
    "\n",
    "def coverage_width(y_true, y_low, y_upper):\n",
    "    coverage = ((y_true >= y_low) & (y_true <= y_upper)).float().mean()\n",
    "    width = (y_upper - y_low).float().abs().mean()\n",
    "    \n",
    "    return coverage, width\n",
    "    \n",
    "def evaluate_model_performance(preds_low, preds_upper, targets, target=0.9):\n",
    "    coverage = np.mean((targets >= preds_low) & (targets <= preds_upper))\n",
    "    \n",
    "    interval_width = np.mean(preds_upper - preds_low)\n",
    "    normalized_interval_width = interval_width / (np.max(targets) - np.min(targets))\n",
    "    \n",
    "    median_pred = (preds_low + preds_upper) / 2    # 신뢰구간 중앙값\n",
    "    mpe = np.mean(np.abs(median_pred - targets)) # 예측 구간 중심이 실제값과 얼마나 가까운지\n",
    "    \n",
    "    sharpness = np.mean(np.square(preds_upper - preds_low)) # 예측 구간의 날카로움: 제곱을 사용해 큰 폭일수록 더 강한 패널티\n",
    "    \n",
    "    alpha = 0.5\n",
    "    penalties = np.where(targets < preds_low, preds_low - targets, np.where(targets > preds_upper, targets - preds_upper, 0))  \n",
    "    winkler = np.mean(interval_width + 2 * alpha * penalties) # 신뢰구간이 실제값을 포함하지 않으면 패널티 적용\n",
    "    \n",
    "    MCT = interval_width * abs(coverage - target)     # 조정된 Coverage Tradeoff 지표 (MCT)\n",
    "\n",
    "    print(f\"예측 관련 - Coverage Rate (CR) ⬆: {coverage:.2f}, Mean Prediction Error (MPE) ⬇: {mpe:.2f}\")\n",
    "    print(f\"구간 관련 - Interval Width (IW) ⬇: {interval_width:.2f}, Sharpness ⬇: {sharpness:.2f}, Winkler Score (WS) ⬇: {winkler:.2f}\")\n",
    "    print(f\"종합 - MisCoverage Trade-off (MCT) ⬇: {MCT:.2f}\")\n",
    "    \n",
    "    # return {\n",
    "    #     \"Coverage Rate (CR)\": coverage,\n",
    "    #     \"Interval Width (IW)\": interval_width,\n",
    "    #     \"Normalized IW\": normalized_interval_width,\n",
    "    #     \"Mean Prediction Error (MPE)\": mpe,\n",
    "    #     \"Sharpness\": sharpness,\n",
    "    #     \"Winkler Score (WS)\": winkler,\n",
    "    #     \"MisCoverage Trade-off (MCT)\": MCT\n",
    "    # }\n",
    "\n",
    "set_seed(1127)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQNN_R(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim+1, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, tau):\n",
    "        x = augment_features(x, tau)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "class GQNN_N(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "class GQNN_D(nn.Module):\n",
    "    # tau와 preds 같이 출력\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc_tau = nn.Linear(hidden_dim, 2)\n",
    "        self.fc_pred = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        taus = torch.sigmoid(self.fc_tau(x))\n",
    "        tau_low = 0.01 + 0.49 * torch.sigmoid(taus[:, 0:1])  # 0.05 ~ 0.5\n",
    "        tau_upper = 0.5 + 0.49 * torch.sigmoid(taus[:, 1:2])  # 0.5 ~ 0.95\n",
    "        \n",
    "        preds = self.fc_pred(x)\n",
    "        # scale = torch.log1p(torch.abs(preds))  # 작을 때는 작은 값, 클 때는 점진적으로 커지는 특징\n",
    "        scale = F.relu(torch.log1p(torch.abs(preds)) + 0.1)  # 최소값 0.1 보장\n",
    "        preds_low = preds - tau_low * scale\n",
    "        preds_upper = preds + tau_upper * scale\n",
    "        \n",
    "        return preds_low, preds_upper, tau_low, tau_upper\n",
    "\n",
    "class GNN_CP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "class QRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_true, tau):\n",
    "        diff = y_true - y_pred\n",
    "        loss = torch.where(diff > 0, tau * diff, (tau - 1) * diff)\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "    \n",
    "class RQRLoss(nn.Module):\n",
    "    def __init__(self, target=0.9, lambda_factor=0.1):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.lf = lambda_factor\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        q1, q2 = preds[:, 0], preds[:, 1]\n",
    "        diff1 = target - q1\n",
    "        diff2 = target - q2\n",
    "        width = q2 - q1\n",
    "        \n",
    "        rqr_loss = torch.maximum(diff1 * diff2 * (self.target + 2 * self.lf),\n",
    "                                diff2 * diff1 * (self.target + 2 * self.lf - 1))\n",
    "        \n",
    "        width_loss = self.lf * torch.square(width) * 0.5\n",
    "        \n",
    "        return torch.mean(rqr_loss + width_loss)\n",
    "    \n",
    "class IQRLoss(nn.Module):\n",
    "    def __init__(self, target_coverage=0.9, gamma_factor=2, lambda_factor=1):\n",
    "        super().__init__()\n",
    "        self.alpha = target_coverage\n",
    "        self.gf = gamma_factor\n",
    "        self.lf = lambda_factor\n",
    "        \n",
    "    def forward(self, preds_low, preds_upper, tau_low, tau_upper, target):\n",
    "        # Quantile Loss\n",
    "        diff1 = target - preds_low\n",
    "        loss1 = torch.where(diff1 > 0, tau_low * diff1, (tau_low - 1) * diff1)\n",
    "        # 하한 손실에 1.5배 가중치: 하한 예측 더 신중하게 학습하도록\n",
    "        # loss1 = torch.where(diff1 > 0, 1.5 * tau_low * diff1, 1.5 * (tau_low - 1) * diff1) \n",
    "        diff2 = target - preds_upper\n",
    "        loss2 = torch.where(diff2 > 0, tau_upper * diff2, (tau_upper - 1) * diff2)\n",
    "        qr_loss = torch.mean(loss1 + loss2)\n",
    "        \n",
    "        # Coverage Loss\n",
    "        coverage_loss = torch.mean(torch.maximum(diff1 * diff2 * (self.alpha + 2 * self.lf), \n",
    "                                                 diff2 * diff1 * (self.alpha + 2 * self.lf - 1)) )\n",
    "        # in_coverage = (preds_low <= target) & (target <= preds_upper)  # 구간 내 포함 여부\n",
    "        # current_coverage = torch.mean(in_coverage.float())  # 현재 커버리지 비율\n",
    "        # coverage_loss = torch.mean(self.gf * torch.square(self.alpha - current_coverage) * 0.5)\n",
    "        \n",
    "        # Width loss\n",
    "        # width_penalty = torch.mean(self.lf * torch.square(preds_upper - preds_low) * 0.5)\n",
    "        # violation_penalty = torch.relu(preds_low - preds_upper).mean() * 10\n",
    "        \n",
    "        loss = qr_loss + coverage_loss \n",
    "        # + width_penalty\n",
    "    \n",
    "        return loss\n",
    "\n",
    "class IQRLossWithCoverage(nn.Module):\n",
    "    def __init__(self, target_coverage=0.9, coverage_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.target_coverage = target_coverage  # 목표 커버리지 (예: 0.9 for 90%)\n",
    "        self.coverage_weight = coverage_weight  # 커버리지 손실 가중치\n",
    "        \n",
    "    def forward(self, preds_low, preds_upper, target, tau_low, tau_upper):\n",
    "        # 기존 Quantile Loss 계산\n",
    "        diff1 = target - preds_low\n",
    "        diff2 = target - preds_upper\n",
    "        loss1 = torch.where(diff1 > 0, tau_low * diff1, (tau_low - 1) * diff1)\n",
    "        loss2 = torch.where(diff2 > 0, tau_upper * diff2, (tau_upper - 1) * diff2)\n",
    "        quantile_loss = torch.mean(loss1 + loss2)\n",
    "        \n",
    "        # 커버리지 계산\n",
    "        in_coverage = (preds_low <= target) & (target <= preds_upper)  # 구간 내 포함 여부\n",
    "        actual_coverage = torch.mean(in_coverage.float())  # 실제 커버리지 비율\n",
    "        \n",
    "        # 커버리지 손실: 목표 커버리지와의 차이를 벌점으로 추가\n",
    "        coverage_loss = torch.abs(self.target_coverage - actual_coverage)\n",
    "        \n",
    "        # 총 손실: Quantile Loss + Coverage Loss\n",
    "        total_loss = quantile_loss + self.coverage_weight * coverage_loss\n",
    "        \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    \"\"\"Bayesian Linear Layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, prior_std=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Mean and log variance of weights\n",
    "        self.w_mu = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.w_logvar = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "\n",
    "        # Mean and log variance of biases\n",
    "        self.b_mu = nn.Parameter(torch.zeros(out_features))\n",
    "        self.b_logvar = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "        # Prior distribution (Gaussian)\n",
    "        self.prior = Normal(0, prior_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample weights and biases\n",
    "        w_std = torch.exp(0.5 * self.w_logvar)\n",
    "        b_std = torch.exp(0.5 * self.b_logvar)\n",
    "\n",
    "        w = self.w_mu + w_std * torch.randn_like(w_std)\n",
    "        b = self.b_mu + b_std * torch.randn_like(b_std)\n",
    "\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "class BayesianGNN(nn.Module):\n",
    "    \"\"\"Bayesian GNN using Bayesian Linear Layer\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = BayesianLinear(hidden_dim, 1)  # Bayesian Linear Layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return self.fc(x).squeeze()  # (batch_size,)\n",
    "    \n",
    "class MCDropoutGNN(nn.Module):\n",
    "    \"\"\"MC Dropout 기반 GNN\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, training=True):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=training)  # Dropout 유지\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=training)  # Dropout 유지\n",
    "        return self.fc(x).squeeze()  # (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2869072/1017196909.py:162: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  pyg_data.x = torch.tensor(feats, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# 기본 비선형 그래프 데이터\n",
    "graph_data_basic = generate_graph_data(num_nodes=1000)\n",
    "\n",
    "# 노이즈 비선형 그래프 데이터\n",
    "graph_data_noise_gaussian = generate_noisy_graph_data(num_nodes=1000, noise_type='gaussian', noise_level=0.3)\n",
    "graph_data_noise_uniform = generate_noisy_graph_data(num_nodes=1000, noise_type='uniform', noise_level=0.3)\n",
    "graph_data_noise_outlier = generate_noisy_graph_data(num_nodes=1000, noise_type='outlier', noise_level=0.3)\n",
    "graph_data_noise_edge = generate_noisy_graph_data(num_nodes=1000, noise_type='edge_noise', noise_level=0.3)\n",
    "\n",
    "# County 데이터셋\n",
    "graph_data_county_edu = load_county_graph_data('education', 2012)\n",
    "graph_data_county_elec = load_county_graph_data('election', 2012)\n",
    "graph_data_county_inc = load_county_graph_data('income', 2012)\n",
    "graph_data_county_unemp = load_county_graph_data('unemployment', 2012)\n",
    "\n",
    "# Twitch 데이터셋\n",
    "# graph_data_twitch_de = load_twitch_graph_data('DE')\n",
    "# graph_data_twitch_engb = load_twitch_graph_data('ENGB')\n",
    "# graph_data_twitch_es = load_twitch_graph_data('ES')\n",
    "# graph_data_twitch_fr = load_twitch_graph_data('FR')\n",
    "graph_data_twitch_ptbr = load_twitch_graph_data('PTBR')\n",
    "# graph_data_twitch_ru = load_twitch_graph_data('RU')\n",
    "\n",
    "# Wikipedia 데이터\n",
    "graph_data_wiki_ch = load_wiki_graph_data('chameleon')\n",
    "graph_data_wiki_cr = load_wiki_graph_data('crocodile')\n",
    "graph_data_wiki_sq = load_wiki_graph_data('squirrel')\n",
    "\n",
    "# Transfortation 데이터셋\n",
    "graph_data_trans_ana = load_trans_graph_data('Anaheim')\n",
    "graph_data_trans_chica = load_trans_graph_data('ChicagoSketch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Nodes: 800, Train Edges: 1287\n",
      "Train edge_index 최대값: 799\n",
      "Test Nodes: 200, Test Edges: 87\n",
      "Test edge_index 최대값: 198\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "train_data, test_data = split_graph_data(graph_data_noise_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Calibration Dataset --------------------\n",
      "Train Nodes: 640, Train Edges: 836\n",
      "Train edge_index 최대값: 639\n",
      "Test Nodes: 160, Test Edges: 55\n",
      "Test edge_index 최대값: 156\n",
      "Train data: 640 nodes, 844 edges\n",
      "Train edge_index 최대값: 639\n",
      "Calibration data: 160 nodes, 52 edges\n",
      "Calibration edge_index 최대값: 159\n",
      "Test data: 200 nodes, 87 edges\n",
      "Test edge_index 최대값: 198\n"
     ]
    }
   ],
   "source": [
    "# Train 데이터에서 min/max 구하기\n",
    "train_min = train_data.x.min()\n",
    "train_max = train_data.x.max()\n",
    "y_min = train_data.y.min()\n",
    "y_max = train_data.y.max()\n",
    "\n",
    "# 정규화 함수\n",
    "def normalize(tensor, min_val, max_val):\n",
    "    return (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "train_data.x = normalize(train_data.x, train_min, train_max)\n",
    "test_data.x = normalize(test_data.x, train_min, train_max)\n",
    "train_data.y = normalize(train_data.y, y_min, y_max)\n",
    "test_data.y = normalize(test_data.y, y_min, y_max)\n",
    "\n",
    "print('-' * 20, 'Calibration Dataset', '-' * 20)\n",
    "cp_train_data, calibration_data = split_graph_data(train_data)\n",
    "num_train = train_data.x.shape[0]\n",
    "num_calibration = int(num_train * 0.2)  # Calibration 비율 20%\n",
    "\n",
    "# 랜덤하게 인덱스 생성\n",
    "indices = torch.randperm(num_train)\n",
    "calibration_indices = indices[:num_calibration]\n",
    "cp_train_indices = indices[num_calibration:]\n",
    "\n",
    "# 노드 마스크 생성\n",
    "cp_train_mask = torch.zeros(num_train, dtype=torch.bool)\n",
    "calibration_mask = torch.zeros(num_train, dtype=torch.bool)\n",
    "cp_train_mask[cp_train_indices] = True\n",
    "calibration_mask[calibration_indices] = True\n",
    "\n",
    "# cp_train용 edge_index 필터링 및 재매핑\n",
    "cp_train_edge_mask = cp_train_mask[train_data.edge_index[0]] & cp_train_mask[train_data.edge_index[1]]\n",
    "cp_train_edge_index = train_data.edge_index[:, cp_train_edge_mask]\n",
    "cp_train_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(cp_train_indices)}\n",
    "cp_train_edge_index = torch.tensor(\n",
    "    [[cp_train_node_map[idx.item()] for idx in cp_train_edge_index[0]],\n",
    "     [cp_train_node_map[idx.item()] for idx in cp_train_edge_index[1]]],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "# calibration용 edge_index 필터링 및 재매핑\n",
    "calibration_edge_mask = calibration_mask[train_data.edge_index[0]] & calibration_mask[train_data.edge_index[1]]\n",
    "calibration_edge_index = train_data.edge_index[:, calibration_edge_mask]\n",
    "calibration_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(calibration_indices)}\n",
    "calibration_edge_index = torch.tensor(\n",
    "    [[calibration_node_map[idx.item()] for idx in calibration_edge_index[0]],\n",
    "     [calibration_node_map[idx.item()] for idx in calibration_edge_index[1]]],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "# cp_train_data 생성\n",
    "cp_train_data = Data(\n",
    "    x=train_data.x[cp_train_indices],\n",
    "    y=train_data.y[cp_train_indices],\n",
    "    edge_index=cp_train_edge_index\n",
    ")\n",
    "\n",
    "# calibration_data 생성\n",
    "calibration_data = Data(\n",
    "    x=train_data.x[calibration_indices],\n",
    "    y=train_data.y[calibration_indices],\n",
    "    edge_index=calibration_edge_index\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Train data: {cp_train_data.x.shape[0]} nodes, {cp_train_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Train edge_index 최대값: {cp_train_data.edge_index.max().item()}\")\n",
    "print(f\"Calibration data: {calibration_data.x.shape[0]} nodes, {calibration_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Calibration edge_index 최대값: {calibration_data.edge_index.max().item()}\")\n",
    "print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "color = sns.color_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQR-GNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "\n",
    "device = torch.device(f\"cuda:1\" if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model = GQNN_R(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "criterion = QRLoss()\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    taus = torch.rand(train_data.x.size(0), 1, dtype=torch.float32, device=device)\n",
    "    preds = model(train_data.x, train_data.edge_index, taus)\n",
    "    loss = criterion(preds, train_data.y, taus)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print(f\"Epoch {epoch}: Quantile Loss = {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color=color[0])\n",
    "plt.title('SQR-GNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "test_data = test_data.to(device)\n",
    "tau_low = 0.05\n",
    "tau_upper = 0.95\n",
    "\n",
    "tau_lows = torch.full((train_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "tau_uppers = torch.full((train_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_low_preds = model(train_data.x, train_data.edge_index, tau_lows).cpu().numpy()\n",
    "    train_upper_preds = model(train_data.x, train_data.edge_index, tau_uppers).cpu().numpy()\n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('SQR-GNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[0], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[0], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "tau_lows = torch.full((test_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "tau_uppers = torch.full((test_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_low_preds = model(test_data.x, test_data.edge_index, tau_lows).cpu().numpy()\n",
    "    test_upper_preds = model(test_data.x, test_data.edge_index, tau_uppers).cpu().numpy()\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('SQR-GNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[1], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[1], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQR-GNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "target=0.9\n",
    "lambda_factor=0.1\n",
    "\n",
    "device = torch.device(f\"cuda:1\" if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model = GQNN_N(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "criterion = RQRLoss(target=target, lambda_factor=lambda_factor)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = model(train_data.x, train_data.edge_index)\n",
    "    loss = criterion(preds, train_data.y)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print(f\"Epoch {epoch}: Quantile Loss = {loss.item():.4f}\")\n",
    "    \n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color=color[1])\n",
    "plt.title('RQR-GNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_preds = model(train_data.x, train_data.edge_index)\n",
    "    train_low_preds = train_preds[:, 0].cpu().numpy()\n",
    "    train_upper_preds = train_preds[:, 1].cpu().numpy()\n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('RQR-GNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[2], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[2], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = model(test_data.x, test_data.edge_index)\n",
    "    test_low_preds = test_preds[:, 0].cpu().numpy()\n",
    "    test_upper_preds = test_preds[:, 1].cpu().numpy()\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('RQR-GNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[3], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[3], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP-GNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "alpha = 0.1  # 신뢰수준 90% (1 - alpha)\n",
    "\n",
    "device = torch.device(f\"cuda:1\" if torch.cuda.is_available() else 'cpu')\n",
    "# device =torch.device(\"cuda\")\n",
    "cp_train_data = cp_train_data.to(device)\n",
    "\n",
    "model = GNN_CP(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = model(cp_train_data.x, cp_train_data.edge_index)\n",
    "    loss = F.mse_loss(preds, cp_train_data.y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color =color[2])\n",
    "plt.title('CP-GNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "calibration_data = calibration_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_cal = model(calibration_data.x, calibration_data.edge_index)\n",
    "    preds_train = model(train_data.x, train_data.edge_index).cpu().numpy()\n",
    "    preds_test = model(test_data.x, test_data.edge_index).cpu().numpy()\n",
    "\n",
    "conformal_scores = torch.abs(calibration_data.y- preds_cal).cpu().numpy()\n",
    "q_hat = np.quantile(conformal_scores, 1 - alpha)\n",
    "\n",
    "train_low_preds = preds_train - q_hat\n",
    "train_upper_preds = preds_train + q_hat\n",
    "train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('CP-GNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[6], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[6], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "test_low_preds = preds_test - q_hat\n",
    "test_upper_preds = preds_test + q_hat\n",
    "test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('CP-GNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[7], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[7], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "num_samples=100\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model_bnn = BayesianGNN(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model_bnn.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model_bnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = model_bnn(train_data.x, train_data.edge_index)\n",
    "    loss = F.mse_loss(preds, train_data.y.squeeze())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color =color[4])\n",
    "plt.title('BayesianNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model_bnn.eval()\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_samples):\n",
    "        preds = model_bnn(train_data.x, train_data.edge_index)  # Bayesian Sampling\n",
    "        preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "train_low_preds = mean_preds - 1.96 * std_preds  # 95% 신뢰구간 하한\n",
    "train_upper_preds = mean_preds + 1.96 * std_preds  # 95% 신뢰구간 상한\n",
    "train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('muted')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('BayesainNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[-1], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[-1], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_samples):\n",
    "        preds = model_bnn(test_data.x, test_data.edge_index)  # Bayesian Sampling\n",
    "        preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "test_low_preds = mean_preds - 1.96 * std_preds  # 95% 신뢰구간 하한\n",
    "test_upper_preds = mean_preds + 1.96 * std_preds  # 95% 신뢰구간 상한\n",
    "test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('BayesianNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[-2], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[-2], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Dropout\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "dropout = 0.2\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model_mc = MCDropoutGNN(in_dim=in_dim, hidden_dim=hidden_dim, dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(model_mc.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model_mc.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds_mc = model_mc(train_data.x, train_data.edge_index, training=True)\n",
    "    loss_mc = F.mse_loss(preds_mc, train_data.y.squeeze())\n",
    "    \n",
    "    loss_mc.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss_mc.item())\n",
    "    \n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color =color[5])\n",
    "plt.title('MCdropout Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model_mc.eval()\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_samples):\n",
    "        preds = model_mc(train_data.x, train_data.edge_index, training=True)  # Dropout 유지\n",
    "        preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "train_low_preds = mean_preds - 1.96 * std_preds  # 95% 신뢰구간 하한\n",
    "train_upper_preds = mean_preds + 1.96 * std_preds  # 95% 신뢰구간 상한\n",
    "train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('muted')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('MCDropout Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[-3], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[-3], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_samples):\n",
    "        preds = model_mc(test_data.x, test_data.edge_index, training=True)  # Dropout 유지\n",
    "        preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "test_low_preds = mean_preds - 1.96 * std_preds  # 95% 신뢰구간 하한\n",
    "qtest_upper_preds2 = mean_preds + 1.96 * std_preds  # 95% 신뢰구간 상한\n",
    "test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('MCDropout Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[-4], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[-4], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGQNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "target=0.9\n",
    "gamma_factor=2\n",
    "lambda_factor=1\n",
    "\n",
    "device = torch.device(f\"cuda:1\" if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model = GQNN_D(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "criterion = IQRLoss(target_coverage=target, gamma_factor=gamma_factor, lambda_factor=lambda_factor)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds_low, preds_upper, taus_low, taus_upper = model(train_data.x, train_data.edge_index)\n",
    "    loss = criterion(preds_low, preds_upper, taus_low, taus_upper, train_data.y)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    cvg, wdt = coverage_width(train_data.y, preds_low, preds_upper)\n",
    "    \n",
    "    if epoch % (num_epochs // 5) == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.2f}, Coverage = {cvg.item():.2f}, Width = {wdt.item():.2f}\")\n",
    "    \n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color =color[3])\n",
    "plt.title('DGQNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_low, preds_upper, _, _ = model(train_data.x, train_data.edge_index)\n",
    "    train_low_preds = preds_low.cpu().numpy()\n",
    "    train_upper_preds = preds_upper.cpu().numpy()\n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('DGQNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[4], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[4], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_low, preds_upper, _, _ = model(test_data.x, test_data.edge_index)\n",
    "    test_low_preds = preds_low.cpu().numpy()\n",
    "    test_upper_preds = preds_upper.cpu().numpy()\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))  \n",
    "fig.suptitle('DGQNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[5], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[5], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "944c0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # 상위 폴더로 이동\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from PyPDF2 import PdfMerger\n",
    "from torch.distributions.normal import Normal\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv, GraphSAGE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch_geometric.transforms import RandomNodeSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6685a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GQNNLoss2' from 'utills.model' (/home/sypark/QpiGNN/training/../utills/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutills\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_seed, generate_graph_data, generate_noisy_graph_data, load_county_graph_data, load_twitch_graph_data, \\\n\u001b[1;32m      2\u001b[0m             load_wiki_graph_data, load_trans_graph_data, create_ba_graph_pyg, create_er_graph_pyg, create_grid_graph_pyg, create_tree_graph_pyg, \\\n\u001b[1;32m      3\u001b[0m             normalize, split_graph_data, split_cp_graph_data, evaluate_model_performance, sort_by_y, coverage_width, \\\n\u001b[1;32m      4\u001b[0m                 get_gpu_memory, get_cpu_memory, count_parameters\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutills\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GQNN_R, GQNN_N, BayesianGNN, MCDropoutGNN, GQNN, QRLoss, RQRLoss, GQNNLoss, GQNNLoss2\n\u001b[1;32m      7\u001b[0m set_seed(\u001b[38;5;241m1127\u001b[39m)  \n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GQNNLoss2' from 'utills.model' (/home/sypark/QpiGNN/training/../utills/model.py)"
     ]
    }
   ],
   "source": [
    "from utills.function import set_seed, generate_graph_data, generate_noisy_graph_data, load_county_graph_data, load_twitch_graph_data, \\\n",
    "            load_wiki_graph_data, load_trans_graph_data, create_ba_graph_pyg, create_er_graph_pyg, create_grid_graph_pyg, create_tree_graph_pyg, \\\n",
    "            normalize, split_graph_data, split_cp_graph_data, evaluate_model_performance, sort_by_y, coverage_width, \\\n",
    "                get_gpu_memory, get_cpu_memory, count_parameters\n",
    "from utills.model import GQNN_R, GQNN_N, BayesianGNN, MCDropoutGNN, GQNN, QRLoss, RQRLoss, GQNNLoss\n",
    "\n",
    "set_seed(1127)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e204808",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'basic'\n",
    "nodes = 1000\n",
    "noise = 0.3\n",
    "target_coverage = 0.9\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "\n",
    "epochs = 500\n",
    "runs = 10\n",
    "lambda_factor = 1\n",
    "num_samples = 100\n",
    "dropout = 0.2\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98369f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset != '':\n",
    "    if dataset == 'basic':\n",
    "        graph_data = generate_graph_data(num_nodes=nodes)\n",
    "    elif dataset in ('gaussian', 'uniform', 'outlier', 'edge'):\n",
    "        graph_data = generate_noisy_graph_data(num_nodes=nodes, noise_type=dataset, noise_level=noise)\n",
    "    elif dataset in ('education', 'election', 'income', 'unemployment'):\n",
    "        graph_data = load_county_graph_data(dataset, 2012)\n",
    "    elif dataset in ('DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU'):\n",
    "        graph_data = load_twitch_graph_data(dataset)\n",
    "    elif dataset in ('chameleon', 'crocodile', 'squirrel'):\n",
    "        graph_data = load_wiki_graph_data(dataset)\n",
    "    elif dataset in ('Anaheim', 'ChicagoSketch'):\n",
    "        graph_data = load_trans_graph_data(dataset)\n",
    "    elif dataset == 'BA':\n",
    "        graph_data = create_ba_graph_pyg(n=nodes)\n",
    "    elif dataset == 'ER':\n",
    "        graph_data = create_er_graph_pyg(n=nodes)\n",
    "    elif dataset == 'grid':\n",
    "        graph_data = create_grid_graph_pyg()\n",
    "    elif dataset == 'tree':\n",
    "        graph_data = create_tree_graph_pyg()\n",
    "    \n",
    "# split data & normalize\n",
    "train_data, test_data = split_graph_data(graph_data, test_ratio=0.2)\n",
    "train_min, train_max, y_min, y_max = train_data.x.min(), train_data.x.max(), train_data.y.min(), train_data.y.max()\n",
    "train_data.x, test_data.x, train_data.y, test_data.y= normalize(train_data.x, train_min, train_max), normalize(test_data.x, train_min, train_max), normalize(train_data.y, y_min, y_max), normalize(test_data.y, y_min, y_max)\n",
    "\n",
    "print(f\"Train data: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Train edge_index 최대값: {train_data.edge_index.max().item()}\")\n",
    "print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "if model == 'CP':\n",
    "    cp_train_data, calibration_data = split_cp_graph_data(train_data, cali_ratio=0.2)\n",
    "\n",
    "    print(f\"Train data: {cp_train_data.x.shape[0]} nodes, {cp_train_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Train edge_index 최대값: {cp_train_data.edge_index.max().item()}\")\n",
    "    print(f\"Calibration data: {calibration_data.x.shape[0]} nodes, {calibration_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Calibration edge_index 최대값: {calibration_data.edge_index.max().item()}\")\n",
    "    print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "# result folder & file\n",
    "root_dir = f\"./pred/{model}/\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "if pdf:\n",
    "    pdf_dir = os.path.join(root_dir, 'img')\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "if optimal:\n",
    "    if dataset in ['basic', 'gaussian', 'uniform', 'outlier', 'edge', 'BA', 'ER', 'grid', 'tree']:\n",
    "        df = pd.read_csv(\"./lambda/syn/lambda_optimized_results.csv\")\n",
    "        optimal_lambda = df[df['Dataset'] == dataset]['Best Lambda'].values[0]\n",
    "    else:\n",
    "        df = pd.read_csv(\"./lambda/real/lambda_optimized_results.csv\")\n",
    "        optimal_lambda = df[df['Dataset'] == dataset]['Best Lambda'].values[0]\n",
    "\n",
    "file_name = dataset + '_' + model\n",
    "if model == 'GQNN':\n",
    "    if optimal:\n",
    "        file_name += f'_lf({optimal_lambda})'\n",
    "    else:\n",
    "        file_name += f'_lf({lambda_factor})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = train_data.x.shape[1]\n",
    "train_data = train_data.to(device)\n",
    "pastel_colors = sns.color_palette('Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e670dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'SQR':\n",
    "    tau_low = (1 - target_coverage)/2\n",
    "    tau_upper = 1 - tau_low\n",
    "    color = pastel_colors[0]\n",
    "    model = GQNN_R(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    criterion = QRLoss()\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats() \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        taus = torch.rand(train_data.x.size(0), 1, dtype=torch.float32, device=device)\n",
    "        preds = model(train_data.x, train_data.edge_index, taus)\n",
    "        loss = criterion(preds, train_data.y, taus)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    gpu_mem = get_gpu_memory()\n",
    "    cpu_mem = get_cpu_memory()\n",
    "    param_count = count_parameters(model)\n",
    "\n",
    "    result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "    result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "    result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "    result_this_run['param_count'] = param_count\n",
    "    \n",
    "    print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "    print('-' * 40, f'{model}: {dataset} Train Evaluation... ', '-' * 40)\n",
    "    model.eval()\n",
    "    tau_lows = torch.full((train_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "    tau_uppers = torch.full((train_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_low_preds = model(train_data.x, train_data.edge_index, tau_lows).cpu().numpy()\n",
    "        train_upper_preds = model(train_data.x, train_data.edge_index, tau_uppers).cpu().numpy()\n",
    "        train_targets = train_data.y.cpu().numpy()\n",
    "    train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=target_coverage)\n",
    "    result_this_run['train_metrics'] = train_eval\n",
    "    \n",
    "    print('-' * 40, f'{model}: {dataset} Test Evaluation... ', '-' * 40)\n",
    "    test_data = test_data.to(device)\n",
    "    tau_lows = torch.full((test_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "    tau_uppers = torch.full((test_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_low_preds = model(test_data.x, test_data.edge_index, tau_lows).cpu().numpy()\n",
    "        test_upper_preds = model(test_data.x, test_data.edge_index, tau_uppers).cpu().numpy()\n",
    "        test_targets = test_data.y.cpu().numpy()\n",
    "    test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=target_coverage)\n",
    "    result_this_run['test_metrics'] = test_eval\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92055663",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif model == 'RQR':\n",
    "    color = pastel_colors[1]\n",
    "    model = GQNN_N(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    criterion = RQRLoss(target=target_coverage, lambda_factor=lambda_factor)  # lambda_factor 고정함\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(train_data.x, train_data.edge_index)\n",
    "        loss = criterion(preds, train_data.y)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    gpu_mem = get_gpu_memory()\n",
    "    cpu_mem = get_cpu_memory()\n",
    "    param_count = count_parameters(model)\n",
    "\n",
    "    result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "    result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "    result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "    result_this_run['param_count'] = param_count\n",
    "    \n",
    "    print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "\n",
    "    print('-' * 40, f'{model}: {dataset} Train Evaluation... ', '-' * 40)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_preds = model(train_data.x, train_data.edge_index)\n",
    "        train_low_preds = train_preds[:, 0].cpu().numpy()\n",
    "        train_upper_preds = train_preds[:, 1].cpu().numpy()\n",
    "        train_targets = train_data.y.cpu().numpy()\n",
    "    train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=target_coverage)\n",
    "    result_this_run['train_metrics'] = train_eval\n",
    "    \n",
    "    print('-' * 40, f'{model}: {dataset} Test Evaluation... ', '-' * 40)\n",
    "    test_data = test_data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(test_data.x, test_data.edge_index)\n",
    "        test_low_preds = test_preds[:, 0].cpu().numpy()\n",
    "        test_upper_preds = test_preds[:, 1].cpu().numpy()\n",
    "        test_targets = test_data.y.cpu().numpy()\n",
    "    test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=target_coverage)\n",
    "    result_this_run['test_metrics'] = test_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elif model == 'BNN':\n",
    "    color = pastel_colors[3]\n",
    "    model = BayesianGNN(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(train_data.x, train_data.edge_index)\n",
    "        loss = F.mse_loss(preds, train_data.y.squeeze())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    gpu_mem = get_gpu_memory()\n",
    "    cpu_mem = get_cpu_memory()\n",
    "    param_count = count_parameters(model)\n",
    "\n",
    "    result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "    result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "    result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "    result_this_run['param_count'] = param_count\n",
    "    \n",
    "    print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "                \n",
    "    print('-' * 40, f'{model}: {dataset} Train Evaluation... ', '-' * 40)\n",
    "    model.eval()\n",
    "\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            preds = model(train_data.x, train_data.edge_index)  \n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "    preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "    mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "    std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "    if target_coverage == 0.9:\n",
    "        t = 1.645\n",
    "    elif target_coverage == 0.95:\n",
    "        t = 1.96\n",
    "    # 80%: 1.28 / 90%: 1.645 / 95%: 1.96 / 99%: 2.576\n",
    "\n",
    "    train_low_preds = mean_preds - t * std_preds  \n",
    "    train_upper_preds = mean_preds + t * std_preds \n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "    train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=target_coverage)\n",
    "    result_this_run['train_metrics'] = train_eval\n",
    "    \n",
    "    print('-' * 40, f'{model}: {dataset} Test Evaluation... ', '-' * 40)\n",
    "    test_data = test_data.to(device)\n",
    "\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            preds = model(test_data.x, test_data.edge_index)  # Bayesian Sampling\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "    preds_array = np.array(preds_list)  \n",
    "    mean_preds = preds_array.mean(axis=0)  \n",
    "    std_preds = preds_array.std(axis=0)    \n",
    "\n",
    "    test_low_preds = mean_preds - t * std_preds \n",
    "    test_upper_preds = mean_preds + t * std_preds \n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "    test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=target_coverage)\n",
    "    result_this_run['test_metrics'] = test_eval\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a43eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif model == 'MC':\n",
    "    color = pastel_colors[4]\n",
    "    model = MCDropoutGNN(in_dim=in_dim, hidden_dim=hidden_dim, dropout=dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(train_data.x, train_data.edge_index, training=True)\n",
    "        loss = F.mse_loss(preds, train_data.y.squeeze())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    gpu_mem = get_gpu_memory()\n",
    "    cpu_mem = get_cpu_memory()\n",
    "    param_count = count_parameters(model)\n",
    "\n",
    "    result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "    result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "    result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "    result_this_run['param_count'] = param_count\n",
    "    \n",
    "    print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "                    \n",
    "    print('-' * 40, f'{model}: {dataset} Train Evaluation... ', '-' * 40)\n",
    "    model.eval()\n",
    "    \n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            preds = model(train_data.x, train_data.edge_index, training=True)  # Dropout 유지\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "    preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "    mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "    std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "    \n",
    "    if target_coverage == 0.9:\n",
    "        t = 1.645\n",
    "    elif target_coverage == 0.95:\n",
    "        t = 1.96\n",
    "        \n",
    "    train_low_preds = mean_preds - t * std_preds \n",
    "    train_upper_preds = mean_preds + t * std_preds \n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "    train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=target_coverage)\n",
    "    result_this_run['train_metrics'] = train_eval\n",
    "    \n",
    "    print('-' * 40, f'{model}: {dataset} Test Evaluation... ', '-' * 40)\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            preds = model(test_data.x, test_data.edge_index, training=True)  # Dropout 유지\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "    preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "    mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "    std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "    test_low_preds = mean_preds - t * std_preds  # 95% 신뢰구간 하한\n",
    "    test_upper_preds = mean_preds + t * std_preds  # 95% 신뢰구간 상한\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "    test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=target_coverage)\n",
    "    result_this_run['test_metrics'] = test_eval\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif model == 'GQNN':\n",
    "    color = pastel_colors[6]\n",
    "    model = GQNN(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    \n",
    "    if optimal:\n",
    "        criterion = GQNNLoss(target_coverage=target_coverage, lambda_factor=optimal_lambda)\n",
    "    else:\n",
    "        criterion = GQNNLoss(target_coverage=target_coverage, lambda_factor=lambda_factor)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "    start_time = time.time()    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds_low, preds_upper = model(train_data.x, train_data.edge_index)\n",
    "        loss = criterion(preds_low, preds_upper, train_data.y)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cvg, wdt = coverage_width(train_data.y, preds_low, preds_upper)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    gpu_mem = get_gpu_memory()\n",
    "    cpu_mem = get_cpu_memory()\n",
    "    param_count = count_parameters(model)\n",
    "\n",
    "    result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "    result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "    result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "    result_this_run['param_count'] = param_count\n",
    "    \n",
    "    print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "    print('-' * 40, f'{model}: {dataset} Train Evaluation... ', '-' * 40)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_low, preds_upper = model(train_data.x, train_data.edge_index)    \n",
    "        train_low_preds = preds_low.cpu().numpy()\n",
    "        train_upper_preds = preds_upper.cpu().numpy()\n",
    "        train_targets = train_data.y.cpu().numpy()\n",
    "    train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=target_coverage)\n",
    "    result_this_run['train_metrics'] = train_eval\n",
    "    \n",
    "    print('-' * 40, f'{model}: {dataset} Test Evaluation... ', '-' * 40)\n",
    "    test_data = test_data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_low, preds_upper = model(test_data.x, test_data.edge_index)    \n",
    "        test_low_preds = preds_low.cpu().numpy()\n",
    "        test_upper_preds = preds_upper.cpu().numpy()\n",
    "        test_targets = test_data.y.cpu().numpy()\n",
    "    test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=target_coverage)\n",
    "    result_this_run['test_metrics'] = test_eval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad272035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

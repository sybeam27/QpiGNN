{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c0df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_pdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfPages\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfMerger\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normal\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import argparse\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from PyPDF2 import PdfMerger///\n",
    "from torch.distributions.normal import Normal\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv, GraphSAGE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch_geometric.transforms import RandomNodeSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6685a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path('../utills/')\n",
    "from utills.function import set_seed, generate_graph_data, generate_noisy_graph_data, load_county_graph_data, load_twitch_graph_data, \\\n",
    "            load_wiki_graph_data, load_trans_graph_data, create_ba_graph_pyg, create_er_graph_pyg, create_grid_graph_pyg, create_tree_graph_pyg, \\\n",
    "            normalize, split_graph_data, split_cp_graph_data, evaluate_model_performance, sort_by_y, coverage_width, \\\n",
    "                get_gpu_memory, get_cpu_memory, count_parameters\n",
    "from utills.model import GQNN_R, GQNN_N, BayesianGNN, MCDropoutGNN, GQNN, QRLoss, RQRLoss, GQNNLoss, GQNNLoss2\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "set_seed(1127)  \n",
    "\n",
    "parser = argparse.ArgumentParser(description='Train GQNN Model')\n",
    "parser.add_argument('--dataset', type=str, default=\"basic\", help='dataset_name')\n",
    "parser.add_argument('--nodes', type=float, default=1000, help='num_nodes')\n",
    "parser.add_argument('--noise', type=float, default=0.3, help='noise_level')\n",
    "parser.add_argument('--target_coverage', type=float, default=0.9, help='target_coverage')\n",
    "\n",
    "parser.add_argument('--model', type=str, default=\"GQNN\", help='model_name')\n",
    "parser.add_argument('--hidden_dim', type=float, default=64, help='hidden_dim')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3, help='learning_rate')\n",
    "parser.add_argument('--weight', type=float, default=1e-3, help='weight_decay')\n",
    "\n",
    "parser.add_argument('--epochs', type=float, default=500, help='num_epochs')\n",
    "parser.add_argument('--runs', type=int, default=10, help='num_runs')\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "parser.add_argument('--pdf', type=bool, default=False, help='pdf_save')\n",
    "parser.add_argument('--optimal', action='store_true', default = False)\n",
    "\n",
    "# parser.add_argument('--tau_low', type=float, default=0.05, help='tau_low')\n",
    "# parser.add_argument('--tau_upper', type=float, default=0.95, help='tau_upper')\n",
    "\n",
    "parser.add_argument('--lambda_factor', type=float, default=1, help='lambda_factor')\n",
    "parser.add_argument('--num_samples', type=float, default=100, help='num_samples')\n",
    "parser.add_argument('--dropout', type=float, default=0.2, help='dropout')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "if args.dataset != '':\n",
    "    if args.dataset == 'basic':\n",
    "        graph_data = generate_graph_data(num_nodes=args.nodes)\n",
    "    elif args.dataset in ('gaussian', 'uniform', 'outlier', 'edge'):\n",
    "        graph_data = generate_noisy_graph_data(num_nodes=args.nodes, noise_type=args.dataset, noise_level=args.noise)\n",
    "    elif args.dataset in ('education', 'election', 'income', 'unemployment'):\n",
    "        graph_data = load_county_graph_data(args.dataset, 2012)\n",
    "    elif args.dataset in ('DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU'):\n",
    "        graph_data = load_twitch_graph_data(args.dataset)\n",
    "    elif args.dataset in ('chameleon', 'crocodile', 'squirrel'):\n",
    "        graph_data = load_wiki_graph_data(args.dataset)\n",
    "    elif args.dataset in ('Anaheim', 'ChicagoSketch'):\n",
    "        graph_data = load_trans_graph_data(args.dataset)\n",
    "    elif args.dataset == 'BA':\n",
    "        graph_data = create_ba_graph_pyg(n=args.nodes)\n",
    "    elif args.dataset == 'ER':\n",
    "        graph_data = create_er_graph_pyg(n=args.nodes)\n",
    "    elif args.dataset == 'grid':\n",
    "        graph_data = create_grid_graph_pyg()\n",
    "    elif args.dataset == 'tree':\n",
    "        graph_data = create_tree_graph_pyg()\n",
    "    \n",
    "# split data & normalize\n",
    "train_data, test_data = split_graph_data(graph_data, test_ratio=0.2)\n",
    "train_min, train_max, y_min, y_max = train_data.x.min(), train_data.x.max(), train_data.y.min(), train_data.y.max()\n",
    "train_data.x, test_data.x, train_data.y, test_data.y= normalize(train_data.x, train_min, train_max), normalize(test_data.x, train_min, train_max), normalize(train_data.y, y_min, y_max), normalize(test_data.y, y_min, y_max)\n",
    "\n",
    "print(f\"Train data: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Train edge_index 최대값: {train_data.edge_index.max().item()}\")\n",
    "print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "if args.model == 'CP':\n",
    "    cp_train_data, calibration_data = split_cp_graph_data(train_data, cali_ratio=0.2)\n",
    "\n",
    "    print(f\"Train data: {cp_train_data.x.shape[0]} nodes, {cp_train_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Train edge_index 최대값: {cp_train_data.edge_index.max().item()}\")\n",
    "    print(f\"Calibration data: {calibration_data.x.shape[0]} nodes, {calibration_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Calibration edge_index 최대값: {calibration_data.edge_index.max().item()}\")\n",
    "    print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "# result folder & file\n",
    "root_dir = f\"./pred/{args.model}/\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "if args.pdf:\n",
    "    pdf_dir = os.path.join(root_dir, 'img')\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "if args.optimal:\n",
    "    if args.dataset in ['basic', 'gaussian', 'uniform', 'outlier', 'edge', 'BA', 'ER', 'grid', 'tree']:\n",
    "        df = pd.read_csv(\"./lambda/syn/lambda_optimized_results.csv\")\n",
    "        optimal_lambda = df[df['Dataset'] == args.dataset]['Best Lambda'].values[0]\n",
    "    else:\n",
    "        df = pd.read_csv(\"./lambda/real/lambda_optimized_results.csv\")\n",
    "        optimal_lambda = df[df['Dataset'] == args.dataset]['Best Lambda'].values[0]\n",
    "\n",
    "file_name = args.dataset + '_' + args.model\n",
    "if args.model == 'GQNN':\n",
    "    if args.optimal:\n",
    "        file_name += f'_lf({optimal_lambda})'\n",
    "    else:\n",
    "        file_name += f'_lf({args.lambda_factor})'\n",
    "     \n",
    "# Training..\n",
    "print('-' * 40, f'{args.model}: {args.dataset} training is starting... ', '-' * 40)\n",
    "\n",
    "in_dim = train_data.x.shape[1]\n",
    "train_data = train_data.to(device)\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "\n",
    "results = {}\n",
    "\n",
    "for run in tqdm(range(args.runs)):\n",
    "    result_this_run = {}\n",
    "    \n",
    "    if args.model == 'SQR':\n",
    "        tau_low = (1 - args.target_coverage)/2\n",
    "        tau_upper = 1 - tau_low\n",
    "        color = pastel_colors[0]\n",
    "        model = GQNN_R(in_dim=in_dim, hidden_dim=args.hidden_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight)\n",
    "        criterion = QRLoss()\n",
    "        \n",
    "        torch.cuda.reset_peak_memory_stats() \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            taus = torch.rand(train_data.x.size(0), 1, dtype=torch.float32, device=device)\n",
    "            preds = model(train_data.x, train_data.edge_index, taus)\n",
    "            loss = criterion(preds, train_data.y, taus)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        cpu_mem = get_cpu_memory()\n",
    "        param_count = count_parameters(model)\n",
    "\n",
    "        result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "        result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "        result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "        result_this_run['param_count'] = param_count\n",
    "        \n",
    "        print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Train Evaluation... ', '-' * 40)\n",
    "        model.eval()\n",
    "        tau_lows = torch.full((train_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "        tau_uppers = torch.full((train_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_low_preds = model(train_data.x, train_data.edge_index, tau_lows).cpu().numpy()\n",
    "            train_upper_preds = model(train_data.x, train_data.edge_index, tau_uppers).cpu().numpy()\n",
    "            train_targets = train_data.y.cpu().numpy()\n",
    "        train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=args.target_coverage)\n",
    "        result_this_run['train_metrics'] = train_eval\n",
    "        \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Test Evaluation... ', '-' * 40)\n",
    "        test_data = test_data.to(device)\n",
    "        tau_lows = torch.full((test_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "        tau_uppers = torch.full((test_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_low_preds = model(test_data.x, test_data.edge_index, tau_lows).cpu().numpy()\n",
    "            test_upper_preds = model(test_data.x, test_data.edge_index, tau_uppers).cpu().numpy()\n",
    "            test_targets = test_data.y.cpu().numpy()\n",
    "        test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=args.target_coverage)\n",
    "        result_this_run['test_metrics'] = test_eval\n",
    "        \n",
    "    elif args.model == 'RQR':\n",
    "        color = pastel_colors[1]\n",
    "        model = GQNN_N(in_dim=in_dim, hidden_dim=args.hidden_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight)\n",
    "        criterion = RQRLoss(target=args.target_coverage, lambda_factor=args.lambda_factor)  # lambda_factor 고정함\n",
    "        \n",
    "        torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "        start_time = time.time()    \n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(train_data.x, train_data.edge_index)\n",
    "            loss = criterion(preds, train_data.y)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        cpu_mem = get_cpu_memory()\n",
    "        param_count = count_parameters(model)\n",
    "\n",
    "        result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "        result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "        result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "        result_this_run['param_count'] = param_count\n",
    "        \n",
    "        print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "\n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Train Evaluation... ', '-' * 40)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_preds = model(train_data.x, train_data.edge_index)\n",
    "            train_low_preds = train_preds[:, 0].cpu().numpy()\n",
    "            train_upper_preds = train_preds[:, 1].cpu().numpy()\n",
    "            train_targets = train_data.y.cpu().numpy()\n",
    "        train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=args.target_coverage)\n",
    "        result_this_run['train_metrics'] = train_eval\n",
    "        \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Test Evaluation... ', '-' * 40)\n",
    "        test_data = test_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_preds = model(test_data.x, test_data.edge_index)\n",
    "            test_low_preds = test_preds[:, 0].cpu().numpy()\n",
    "            test_upper_preds = test_preds[:, 1].cpu().numpy()\n",
    "            test_targets = test_data.y.cpu().numpy()\n",
    "        test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=args.target_coverage)\n",
    "        result_this_run['test_metrics'] = test_eval\n",
    "        \n",
    "    elif args.model == 'BNN':\n",
    "        color = pastel_colors[3]\n",
    "        model = BayesianGNN(in_dim=in_dim, hidden_dim=args.hidden_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight)\n",
    "        \n",
    "        torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "        start_time = time.time()    \n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(train_data.x, train_data.edge_index)\n",
    "            loss = F.mse_loss(preds, train_data.y.squeeze())\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        cpu_mem = get_cpu_memory()\n",
    "        param_count = count_parameters(model)\n",
    "\n",
    "        result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "        result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "        result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "        result_this_run['param_count'] = param_count\n",
    "        \n",
    "        print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "                 \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Train Evaluation... ', '-' * 40)\n",
    "        model.eval()\n",
    "\n",
    "        preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(args.num_samples):\n",
    "                preds = model(train_data.x, train_data.edge_index)  \n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "        preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "        mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "        std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "        if args.target_coverage == 0.9:\n",
    "            t = 1.645\n",
    "        elif args.target_coverage == 0.95:\n",
    "            t = 1.96\n",
    "        # 80%: 1.28 / 90%: 1.645 / 95%: 1.96 / 99%: 2.576\n",
    "\n",
    "        train_low_preds = mean_preds - t * std_preds  \n",
    "        train_upper_preds = mean_preds + t * std_preds \n",
    "        train_targets = train_data.y.cpu().numpy()\n",
    "        train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=args.target_coverage)\n",
    "        result_this_run['train_metrics'] = train_eval\n",
    "        \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Test Evaluation... ', '-' * 40)\n",
    "        test_data = test_data.to(device)\n",
    "\n",
    "        preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(args.num_samples):\n",
    "                preds = model(test_data.x, test_data.edge_index)  # Bayesian Sampling\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "        preds_array = np.array(preds_list)  \n",
    "        mean_preds = preds_array.mean(axis=0)  \n",
    "        std_preds = preds_array.std(axis=0)    \n",
    "\n",
    "        test_low_preds = mean_preds - t * std_preds \n",
    "        test_upper_preds = mean_preds + t * std_preds \n",
    "        test_targets = test_data.y.cpu().numpy()\n",
    "        test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=args.target_coverage)\n",
    "        result_this_run['test_metrics'] = test_eval\n",
    "        \n",
    "    elif args.model == 'MC':\n",
    "        color = pastel_colors[4]\n",
    "        model = MCDropoutGNN(in_dim=in_dim, hidden_dim=args.hidden_dim, dropout=args.dropout).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight)\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "        start_time = time.time()    \n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(train_data.x, train_data.edge_index, training=True)\n",
    "            loss = F.mse_loss(preds, train_data.y.squeeze())\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        cpu_mem = get_cpu_memory()\n",
    "        param_count = count_parameters(model)\n",
    "\n",
    "        result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "        result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "        result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "        result_this_run['param_count'] = param_count\n",
    "        \n",
    "        print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "\n",
    "                       \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Train Evaluation... ', '-' * 40)\n",
    "        model.eval()\n",
    "        \n",
    "        preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(args.num_samples):\n",
    "                preds = model(train_data.x, train_data.edge_index, training=True)  # Dropout 유지\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "        preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "        mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "        std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "        \n",
    "        if args.target_coverage == 0.9:\n",
    "            t = 1.645\n",
    "        elif args.target_coverage == 0.95:\n",
    "            t = 1.96\n",
    "            \n",
    "        train_low_preds = mean_preds - t * std_preds \n",
    "        train_upper_preds = mean_preds + t * std_preds \n",
    "        train_targets = train_data.y.cpu().numpy()\n",
    "        train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=args.target_coverage)\n",
    "        result_this_run['train_metrics'] = train_eval\n",
    "        \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Test Evaluation... ', '-' * 40)\n",
    "        test_data = test_data.to(device)\n",
    "        \n",
    "        preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(args.num_samples):\n",
    "                preds = model(test_data.x, test_data.edge_index, training=True)  # Dropout 유지\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "\n",
    "        preds_array = np.array(preds_list)  # (num_samples, num_nodes)\n",
    "        mean_preds = preds_array.mean(axis=0)  # 평균 예측값\n",
    "        std_preds = preds_array.std(axis=0)    # 표준편차\n",
    "\n",
    "        test_low_preds = mean_preds - t * std_preds  # 95% 신뢰구간 하한\n",
    "        test_upper_preds = mean_preds + t * std_preds  # 95% 신뢰구간 상한\n",
    "        test_targets = test_data.y.cpu().numpy()\n",
    "        test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=args.target_coverage)\n",
    "        result_this_run['test_metrics'] = test_eval\n",
    "        \n",
    "    elif args.model == 'GQNN':\n",
    "        color = pastel_colors[6]\n",
    "        model = GQNN(in_dim=in_dim, hidden_dim=args.hidden_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight)\n",
    "        \n",
    "        if args.optimal:\n",
    "            criterion = GQNNLoss(target_coverage=args.target_coverage, lambda_factor=optimal_lambda)\n",
    "        else:\n",
    "            criterion = GQNNLoss(target_coverage=args.target_coverage, lambda_factor=args.lambda_factor)\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "        start_time = time.time()    \n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds_low, preds_upper = model(train_data.x, train_data.edge_index)\n",
    "            loss = criterion(preds_low, preds_upper, train_data.y)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cvg, wdt = coverage_width(train_data.y, preds_low, preds_upper)\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        cpu_mem = get_cpu_memory()\n",
    "        param_count = count_parameters(model)\n",
    "\n",
    "        result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "        result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "        result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "        result_this_run['param_count'] = param_count\n",
    "        \n",
    "        print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "    \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Train Evaluation... ', '-' * 40)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_low, preds_upper = model(train_data.x, train_data.edge_index)    \n",
    "            train_low_preds = preds_low.cpu().numpy()\n",
    "            train_upper_preds = preds_upper.cpu().numpy()\n",
    "            train_targets = train_data.y.cpu().numpy()\n",
    "        train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=args.target_coverage)\n",
    "        result_this_run['train_metrics'] = train_eval\n",
    "        \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Test Evaluation... ', '-' * 40)\n",
    "        test_data = test_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_low, preds_upper = model(test_data.x, test_data.edge_index)    \n",
    "            test_low_preds = preds_low.cpu().numpy()\n",
    "            test_upper_preds = preds_upper.cpu().numpy()\n",
    "            test_targets = test_data.y.cpu().numpy()\n",
    "        test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=args.target_coverage)\n",
    "        result_this_run['test_metrics'] = test_eval\n",
    "\n",
    "    elif args.model == 'GQNN_2':\n",
    "        color = pastel_colors[7]\n",
    "        model = GQNN(in_dim=in_dim, hidden_dim=args.hidden_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight)\n",
    "        criterion = GQNNLoss2(target_coverage=args.target_coverage, lambda_width=args.lambda_factor) # 고정\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()  # GPU peak 메모리 초기화\n",
    "        start_time = time.time()    \n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds_low, preds_upper = model(train_data.x, train_data.edge_index)\n",
    "            loss = criterion(preds_low, preds_upper, train_data.y)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cvg, wdt = coverage_width(train_data.y, preds_low, preds_upper)\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        gpu_mem = get_gpu_memory()\n",
    "        cpu_mem = get_cpu_memory()\n",
    "        param_count = count_parameters(model)\n",
    "\n",
    "        result_this_run['training_time_sec'] = round(training_time, 2)\n",
    "        result_this_run['gpu_mem_MB'] = round(gpu_mem, 2)\n",
    "        result_this_run['cpu_mem_MB'] = round(cpu_mem, 2)\n",
    "        result_this_run['param_count'] = param_count\n",
    "        \n",
    "        print(f\"Training Time: {training_time:.2f}s | GPU Peak: {gpu_mem:.1f}MB | CPU: {cpu_mem:.1f}MB | Params: {param_count:,}\")\n",
    "         \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Train Evaluation... ', '-' * 40)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_low, preds_upper = model(train_data.x, train_data.edge_index)    \n",
    "            train_low_preds = preds_low.cpu().numpy()\n",
    "            train_upper_preds = preds_upper.cpu().numpy()\n",
    "            train_targets = train_data.y.cpu().numpy()\n",
    "        train_eval = evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=args.target_coverage)\n",
    "        result_this_run['train_metrics'] = train_eval\n",
    "        \n",
    "        print('-' * 40, f'{args.model}: {args.dataset} Test Evaluation... ', '-' * 40)\n",
    "        test_data = test_data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_low, preds_upper = model(test_data.x, test_data.edge_index)    \n",
    "            test_low_preds = preds_low.cpu().numpy()\n",
    "            test_upper_preds = preds_upper.cpu().numpy()\n",
    "            test_targets = test_data.y.cpu().numpy()\n",
    "        test_eval = evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=args.target_coverage)\n",
    "        result_this_run['test_metrics'] = test_eval\n",
    "        \n",
    "    if args.pdf:\n",
    "        existing_pdf = os.path.join(pdf_dir, f\"{args.model}_eval_plots.pdf\")\n",
    "        new_pdf = os.path.join(pdf_dir, \"new_train_plot.pdf\")\n",
    "        merged_pdf = os.path.join(pdf_dir, \"merged_train_plots.pdf\")\n",
    "\n",
    "        x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "        \n",
    "        with PdfPages(new_pdf) as pdf:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 4))  \n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\") \n",
    "            plt.suptitle(f\"Model: {args.model}, Dataset: {args.dataset}, Time: {timestamp} (Train)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "            axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "            axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=color, alpha=0.5)\n",
    "            axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "            axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "            axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "            axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=color, alpha=0.5)\n",
    "            axes[1].set_xlabel(\"Node Index\")  \n",
    "            axes[1].set_ylabel(\"Values\") \n",
    "            \n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "        if os.path.exists(existing_pdf):\n",
    "            merger = PdfMerger()\n",
    "            merger.append(existing_pdf)\n",
    "            merger.append(new_pdf)\n",
    "            merger.write(merged_pdf)\n",
    "            merger.close()\n",
    "\n",
    "            os.replace(merged_pdf, existing_pdf)\n",
    "            os.remove(new_pdf)\n",
    "        else:\n",
    "            os.replace(new_pdf, existing_pdf)\n",
    "            \n",
    "        x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "        \n",
    "        with PdfPages(new_pdf) as pdf:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 4))  \n",
    "            plt.suptitle(f\"Model: {args.model}, Dataset: {args.dataset}, Time: {timestamp} (Test)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "            axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "            axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=color, alpha=0.5)\n",
    "            axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "            axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "            axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "            axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=color, alpha=0.5)\n",
    "            axes[1].set_xlabel(\"Node Index\")  \n",
    "            axes[1].set_ylabel(\"Values\") \n",
    "            \n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "        if os.path.exists(existing_pdf):\n",
    "            merger = PdfMerger()\n",
    "            merger.append(existing_pdf)\n",
    "            merger.append(new_pdf)\n",
    "            merger.write(merged_pdf)\n",
    "            merger.close()\n",
    "\n",
    "            os.replace(merged_pdf, existing_pdf)\n",
    "            os.remove(new_pdf)\n",
    "        else:\n",
    "            os.replace(new_pdf, existing_pdf)\n",
    "    \n",
    "    results[run] = result_this_run\n",
    "    print(f'Finished training {run} run!')\n",
    "\n",
    "print('Saving results to', root_dir + file_name +'.pkl')\n",
    "with open(root_dir + file_name +'.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad272035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saa_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
